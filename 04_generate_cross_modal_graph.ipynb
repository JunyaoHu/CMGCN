{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a9d838e-560f-454a-88e8-b62c30bebfd5",
   "metadata": {},
   "source": [
    "# Step5.3: get_VITfeats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb2db12-2eee-40b4-891e-50757c45cbbd",
   "metadata": {},
   "source": [
    "运行它获得cross_modal_graph，为train做准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286bee1-5598-47f7-88bc-f1996ccbf37e",
   "metadata": {},
   "source": [
    "书接上回，`dic`的格式如下：\n",
    "\n",
    "```\n",
    "{\n",
    "    '682716753374351360': {\n",
    "        'green wall': tensor([-0.2158, -0.5861, -0.4583,  ..., -0.4584, -0.0507, -0.4340]), \n",
    "        'thumb': tensor([-0.0943, -0.5097, -0.2850,  ..., -0.2975,  0.4255,  0.7431]), \n",
    "        'yellow circle': tensor([-0.3630,  0.0496, -0.5468,  ..., -0.5510, -0.0705, -0.4504]), \n",
    "        'blue lights': tensor([-0.4361, -0.5627, -0.6589,  ..., -0.6676, -0.3880, -0.6415]), \n",
    "        'clear glass': tensor([-0.3609, -0.6426, -0.6136,  ..., -0.6126, -0.5325, -0.5997]), \n",
    "        'hand': tensor([-0.4882, -0.5795, -0.6744,  ..., -0.6797, -0.4926, -0.5218]), \n",
    "        'blue dot': tensor([-0.0286, -0.0159, -0.2335,  ..., -0.2459, -0.5378,  0.2008]), \n",
    "        'man': tensor([-0.4034, -0.9045, -0.6479,  ..., -0.6523, -0.0525, -0.8093]), \n",
    "        'white words': tensor([-0.4899,  0.1618, -0.7007,  ..., -0.7047, -0.7450, -0.6402])\n",
    "    }, \n",
    "    '682721949072625664': {\n",
    "        'logo': tensor([ 0.1612,  0.3313, -0.0857,  ..., -0.1012,  0.2761,  0.1852]), \n",
    "        'white logo': tensor([ 0.0115, -0.3251, -0.1830,  ..., -0.1912,  0.7098, -0.2490]), \n",
    "        'blue sky': tensor([-0.1040,  0.2694, -0.2814,  ..., -0.2938, -0.1567,  0.1455]), \n",
    "        'white dog': tensor([-0.1709,  0.2743, -0.3800,  ..., -0.3780,  0.4276, -0.6060]), \n",
    "        'picture': tensor([ 0.1025,  0.3527, -0.0880,  ..., -0.0990,  0.7861, -0.1227]), \n",
    "        'blue wall': tensor([ 0.0800,  0.5311, -0.1281,  ..., -0.1368,  0.2102, -0.1506]), \n",
    "        'black letter': tensor([-0.2343, -0.0206, -0.4577,  ..., -0.4666,  0.2765,  0.2039]), \n",
    "        'green wall': tensor([-0.3552, -0.3162, -0.5775,  ..., -0.5958, -0.9602, -0.4342])\n",
    "    }, \n",
    "    '682722242111774720': {\n",
    "    ...\n",
    "    },\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ecd2ff-c060-4921-a1f0-1d3023cafb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading BertTokenizer.from_pretrained ./bert-base-uncased\n",
      "os.path.exists(url_or_filename) ./bert-base-uncased/vocab.txt\n",
      "2400\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from pytorch_pretrained import BertTokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./bert-base-uncased\")\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "y = 3\n",
    "boxes = {}\n",
    "\n",
    "block = 1\n",
    "\n",
    "for i in range(block):\n",
    "    with open(f\"/root/autodl-tmp/vit_features.B32.finetuned.{i:02}.pkl\",'rb') as fin:\n",
    "        boxes_tmp = pickle.load(fin)\n",
    "        boxes.update(boxes_tmp)\n",
    "del boxes_tmp\n",
    "print(len(boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97e81e9-cfbc-4769-a3d6-808d6cb65111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_adj_matrix(text, aspect, senticNet):\n",
    "    word_list = text.split()\n",
    "    seq_len = len(word_list)\n",
    "    matrix = np.zeros((seq_len, seq_len)).astype('float32')\n",
    "    \n",
    "    for i in range(seq_len):\n",
    "        word = word_list[i]\n",
    "        if word in senticNet:\n",
    "            sentic = float(senticNet[word]) + 1.0\n",
    "        else:\n",
    "            sentic = 0\n",
    "        if word in aspect:\n",
    "            sentic += 1.0\n",
    "        for j in range(seq_len):\n",
    "            matrix[i][j] += sentic\n",
    "            matrix[j][i] += sentic\n",
    "    for i in range(seq_len):\n",
    "        if matrix[i][i] == 0:\n",
    "            matrix[i][i] = 1\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12a4d16-f599-4a45-8e64-aa07437a0c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_adj_matrix_2(text):\n",
    "    doc = nlp(text)\n",
    "    mat = defaultdict(list,[])\n",
    "    for t in doc:\n",
    "        for child in t.children:\n",
    "            mat[child.i].append(t.i)\n",
    "            mat[t.i].append(child.i)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "844bc4eb-be8d-407d-8bdd-bde78429f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load senticNet load_sentic_word():\n",
    "\n",
    "path = './senticNet/senticnet_word.txt'\n",
    "senticNet = {}\n",
    "fp = open(path, 'r')\n",
    "for line in fp:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    word, sentic = line.split('\\t')\n",
    "    senticNet[word] = float(sentic)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca1703e-ae37-4a0a-88f9-caf5732590a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentic_score(word_i,word_j):\n",
    "    if word_i not in senticNet or word_j not in senticNet or word_i == word_j:\n",
    "        return 0\n",
    "    return abs(float(senticNet[word_i] - senticNet[word_j])) * y**(-1*senticNet[word_i]*senticNet[word_j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2566882e-b0ed-45a0-9468-b996f4da9539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(line):\n",
    "    line = line.lower().strip()\n",
    "    bert_token = tokenizer.tokenize(line)\n",
    "    document = nlp(line)\n",
    "    spacy_token = [str(x) for x in document]\n",
    "    \n",
    "    # spacy_token = []\n",
    "    # for x in document:\n",
    "    #     x = str(x).strip()\n",
    "    #     if x != '':\n",
    "    #         spacy_token.append(x)\n",
    "    \n",
    "    spacy_len = len(spacy_token)\n",
    "    bert_len = len(bert_token)\n",
    "    outter_graph = np.zeros((bert_len, bert_len)).astype('float32')\n",
    "    split_link = [None]*spacy_len\n",
    "    \n",
    "    # print(f\"bert_token:  {bert_token}\")\n",
    "    # print(f\"spacy_token: {spacy_token}\")\n",
    "    \n",
    "    \"\"\"\n",
    "    bert_token:  ['we', 'must', 'rebuild', 'our', 'military', '!', '!', '!', 'we', 'need', 'more', 'battleships', '!', '#', 'go', '##pd', '##eb', '##ate']\n",
    "    spacy_token: ['we', 'must', 'rebuild', 'our', 'military', '!', '!', '!', 'we', 'need', 'more', 'battleships', '!', '#', 'gopdebate']\n",
    "    \"\"\"\n",
    "    \n",
    "    # 这里应该是为了对齐分词 两种分词的方式是不一样的\n",
    "    ii = 0\n",
    "    jj = 0\n",
    "    pre = []\n",
    "    s = \"\"\n",
    "    while ii<bert_len and jj < spacy_len:\n",
    "        bert_ = bert_token[ii].replace(\"##\",\"\")\n",
    "        spacy_ = spacy_token[jj]\n",
    "        s += bert_\n",
    "        pre.append(ii)\n",
    "        if s == spacy_:\n",
    "            split_link[jj] = deepcopy(pre)\n",
    "            pre = []\n",
    "            s = \"\"\n",
    "            jj += 1        \n",
    "        ii += 1\n",
    "    \n",
    "    # 暂时想不到别的办法了\n",
    "    # 这种行不通的 长度变了 不能变长度\n",
    "    # split_link = list(filter(lambda x: (x is not None) and (x != \"\"), split_link))\n",
    "    # spacy_len = len(split_link)\n",
    "    # bert_len = len(split_link)\n",
    "    \n",
    "    # for i in range(len(split_link)):\n",
    "    #     if split_link[i] is None:\n",
    "    #         split_link[i] = '[PAD]'\n",
    "            \n",
    "        \n",
    "    flag = False\n",
    "    \n",
    "    mat = dependency_adj_matrix_2(line)\n",
    "    if not(ii<bert_len or jj < spacy_len):\n",
    "        for key,linked in mat.items():\n",
    "            try:\n",
    "                for x in split_link[int(key)]:\n",
    "                    for link in linked:\n",
    "                        for y in split_link[int(link)]:\n",
    "                            outter_graph[x][y] = 1     \n",
    "            except:\n",
    "                flag = True\n",
    "                break\n",
    "    else:\n",
    "        flag = True\n",
    "        \n",
    "    if flag:\n",
    "        tokens = spacy_token\n",
    "        outter_graph = np.zeros((spacy_len, spacy_len)).astype('float32')\n",
    "        inner_graph = np.identity(spacy_len).astype('float32')\n",
    "        doc = nlp(line)\n",
    "        for token in doc:\n",
    "            for child in token.children:\n",
    "                outter_graph[token.i][child.i] = 1\n",
    "                outter_graph[child.i][token.i] = 1    \n",
    "    else:\n",
    "        tokens = bert_token\n",
    "        inner_graph = np.identity(bert_len).astype('float32')\n",
    "        for link in split_link:\n",
    "            for x in link:\n",
    "                for y in link:\n",
    "                    inner_graph[x][y] = 1\n",
    "\n",
    "    \n",
    "    outter_graph = np.pad(outter_graph,((1,1),(1,1)),'constant')\n",
    "    inner_graph = np.pad(inner_graph,((1,1),(1,1)),'constant')\n",
    "    inner_graph[0][0] = 1\n",
    "    inner_graph[-1][-1] = 1\n",
    "    graph_sum = inner_graph + outter_graph\n",
    "    for i in range(len(graph_sum)):\n",
    "        for j in range(len(graph_sum)):\n",
    "            if graph_sum[i][j] > 0:\n",
    "                graph_sum[i][j] = 1\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    return graph_sum,tokens,flag\n",
    "\n",
    "# flag == True   ->  spacy_token \n",
    "# flag == False  ->  bert_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03fcd3ba-57bc-45d6-86b7-4fe959525b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(line):\n",
    "    line = line.lower().strip()\n",
    "    bert_token = tokenizer.tokenize(line)\n",
    "    document = nlp(line)\n",
    "    spacy_token = [str(x) for x in document]\n",
    "    \n",
    "    # spacy_token = []\n",
    "    # for x in document:\n",
    "    #     x = str(x).strip()\n",
    "    #     if x != '':\n",
    "    #         spacy_token.append(x)\n",
    "    \n",
    "    spacy_len = len(spacy_token)\n",
    "    bert_len = len(bert_token)\n",
    "    outter_graph = np.zeros((bert_len, bert_len)).astype('float32')\n",
    "    split_link = [None]*spacy_len\n",
    "    \n",
    "    ii = 0\n",
    "    jj = 0\n",
    "    pre = []\n",
    "    s = \"\"\n",
    "    while ii < bert_len and jj < spacy_len:\n",
    "        bert_ = bert_token[ii].replace(\"##\",\"\")\n",
    "        spacy_ = spacy_token[jj]\n",
    "        s += bert_\n",
    "        pre.append(ii)\n",
    "        if s == spacy_:\n",
    "            split_link[jj] = deepcopy(pre)\n",
    "            pre = []\n",
    "            s = \"\"\n",
    "            jj += 1        \n",
    "        ii += 1\n",
    "        \n",
    "    mat = dependency_adj_matrix_2(line)\n",
    "    if not(ii<bert_len or jj < spacy_len):\n",
    "        for key,linked in mat.items():\n",
    "            try:\n",
    "                for x in split_link[int(key)]:\n",
    "                    for link in linked:\n",
    "                        for y in split_link[int(link)]:\n",
    "                            outter_graph[x][y] = 1     \n",
    "            except:\n",
    "                break\n",
    "    return split_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a810d3a1-be8c-4611-86ad-e7d987c0c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_graph(text,box_texts,flag):\n",
    "    box_tokens = [list(generate_graph(text))+[text] for text in box_texts]\n",
    "    box_tokens = [x[1:] for x in box_tokens]\n",
    "    for idx,val in enumerate(box_tokens):\n",
    "        box_tokens[idx] = [val[0][1:-1]] + val[1:]\n",
    "    box_texts_len = sum([len(x[0]) for x in box_tokens])\n",
    "    bert_text_tokens = tokenizer.tokenize(text.lower())\n",
    "    spacy_text_tokens = nlp(text.lower())\n",
    "    spacy_text_tokens = [str(x).lower() for x in spacy_text_tokens]\n",
    "\n",
    "    flags = []\n",
    "    word = []\n",
    "    \n",
    "    # flag == True   ->  spacy_token \n",
    "    # flag == False  ->  bert_token\n",
    "    \n",
    "    if flag:\n",
    "        graph = np.zeros((len(spacy_text_tokens), (box_texts_len))).astype('float32')\n",
    "        for i,token_i in enumerate(spacy_text_tokens):\n",
    "            cur = 0\n",
    "            si = wn.synsets(token_i)\n",
    "            if len(si) == 0:\n",
    "                continue\n",
    "            si = si[0]\n",
    "            for tokens,flag,text in box_tokens:\n",
    "                flags.append(flag)\n",
    "                if flag:\n",
    "                    for j,token_j in enumerate(tokens):\n",
    "                        sj = wn.synsets(token_j)\n",
    "                        if i == 0:\n",
    "                            word.append([token_j])\n",
    "                        if len(sj) == 0:\n",
    "                            cur += 1\n",
    "                            continue\n",
    "                        sj = sj[0]\n",
    "                        graph[i][cur] = wn.path_similarity(si,sj) + get_sentic_score(si,sj)\n",
    "                        cur += 1\n",
    "                        \n",
    "                else:\n",
    "                    split_link1 = get_split(text)\n",
    "                    tokens_ = nlp(text)\n",
    "                    tokens_ = [str(x) for x in tokens_]\n",
    "                    \n",
    "                    for j,token_j in enumerate(tokens_):\n",
    "                        sj = wn.synsets(token_j)\n",
    "                        if i == 0:\n",
    "                            tmp = []\n",
    "                        for t in split_link1[j]:\n",
    "                            if i == 0:\n",
    "                                tmp.append(tokens[j])\n",
    "                            if len(sj) == 0:\n",
    "                                cur += 1\n",
    "                                continue\n",
    "                            sj_ = sj[0]\n",
    "                            graph[i][cur] = wn.path_similarity(si,sj_) + get_sentic_score(si,sj_)\n",
    "                            cur += 1\n",
    "                        if i == 0:\n",
    "                            word.append(tmp)\n",
    "                        if len(sj) == 0:\n",
    "                            continue\n",
    "\n",
    "    else:\n",
    "        graph = np.zeros((len(bert_text_tokens), (box_texts_len))).astype('float32')\n",
    "        split_links_i_ = get_split(text)\n",
    "        split_links_i = dict()\n",
    "        for idx,value in enumerate(split_links_i_):\n",
    "            for v in value:\n",
    "                split_links_i[bert_text_tokens[v]] = spacy_text_tokens[idx]\n",
    "        for i,token in enumerate(bert_text_tokens):\n",
    "            cur = 0\n",
    "            token = split_links_i[token]\n",
    "            si = wn.synsets(token)\n",
    "            if len(si) == 0:\n",
    "                continue\n",
    "            si = si[0]\n",
    "            \n",
    "            for tokens,flag,text in box_tokens:\n",
    "                    flags.append(flag)\n",
    "                    if flag:\n",
    "                        for j,token_j in enumerate(tokens):\n",
    "                            sj = wn.synsets(token_j)\n",
    "                            if i == 0:\n",
    "                                word.append([token_j])\n",
    "                            if len(sj) == 0:\n",
    "                                cur += 1\n",
    "                                continue\n",
    "                            sj = sj[0]\n",
    "                            graph[i][cur] = wn.path_similarity(si,sj) + get_sentic_score(si,sj)\n",
    "                            cur += 1            \n",
    "                    else:\n",
    "                        split_link1 = get_split(text)\n",
    "                        tokens_ = nlp(text)\n",
    "                        tokens_ = [str(x) for x in tokens_]\n",
    "                        if i == 0:\n",
    "                            tmp = []\n",
    "                        for j,token_j in enumerate(tokens_):\n",
    "                            sj = wn.synsets(token_j)\n",
    "                            for t in split_link1[j]:\n",
    "                                if i == 0:\n",
    "                                    tmp.append(tokens[j])\n",
    "                                if len(sj) == 0:\n",
    "                                    cur += 1\n",
    "                                    continue\n",
    "                                sj_ = sj[0]\n",
    "                                graph[i][cur] = wn.path_similarity(si,sj_) + get_sentic_score(si,sj_)\n",
    "                                cur += 1\n",
    "                                \n",
    "                            if i == 0:\n",
    "                                word.append(tmp)\n",
    "                            if len(sj) == 0:\n",
    "                                continue\n",
    "    return np.pad(graph,((1,1),(1,1)),'constant'),flags,word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9298cf4c-a3ef-4893-a256-02a5dd893908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "images = pd.read_pickle(\"./bottom-up-attention/boxes.pkl\")\n",
    "  \n",
    "def get_real_label(label):\n",
    "    # [attr1 attr2 class1 class2]  -> [attr1 attr2_class1 class2]\n",
    "    label_list = label.split()\n",
    "    size = len(label_list)\n",
    "    for i in range(size-1, 0,-1):\n",
    "        if ' '.join(label_list[:i]) in attributes:\n",
    "            if len(label_list[:i]) > 1:\n",
    "                print(f\"{' '.join(label_list[:i])}_{' '.join(label_list[i:])}\")\n",
    "            return f\"{' '.join(label_list[:i])}_{' '.join(label_list[i:])}\"\n",
    "    else:\n",
    "        # print(\"no attr\")\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5cf89518-84dc-4de5-8c43-c7bdbce9ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# labels_map=json.load(open('labels_map.txt'))\n",
    "# labels_map=[labels_map[str(i)] for i in range(1000)]\n",
    "# labels_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "353f7879-3e03-4300-a84e-bc38038b192a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1601"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_path = './bottom-up-attention/data/genome/1600-400-20'\n",
    "\n",
    "# Load classes\n",
    "classes = ['__background__']\n",
    "with open(os.path.join(data_path, 'objects_vocab.txt')) as f:\n",
    "    for object in f.readlines():\n",
    "        classes.append(object.split(',')[0].lower().strip())\n",
    "\n",
    "# Load attributes\n",
    "attributes = ['__no_attribute__']\n",
    "with open(os.path.join(data_path, 'attributes_vocab.txt')) as f:\n",
    "    for att in f.readlines():\n",
    "        attributes.append(att.split(',')[0].lower().strip())\n",
    "        \n",
    "classes_to_index = {}\n",
    "for i in range(len(classes)):         \n",
    "    classes_to_index[classes[i]] = i\n",
    "len(classes_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "15c0636c-40a5-4a97-83c8-3b4f5c120ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1114"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_to_index['wipers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "206e665d-a1af-4684-8c88-15739a174987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def keep_one_space(text):\n",
    "    regex = re.compile(r\"\\s+\")\n",
    "    text = regex.sub(' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3dad8fc-c43b-4263-b76f-ee8a186bcba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29040/29040 [34:48<00:00, 13.91it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.cleaned : 1970 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2409/2409 [03:38<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.cleaned : 208 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2410/2410 [04:19<00:00,  9.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid.cleaned : 222 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "['910308516510011393', 'most # funny quotes : 21 snarky and # funny quotes  # # funnyquotes # hilariousquotes # humor # lol ... <url> …', 1]\n",
    "['725333760762363905', ' spurs # creativethinking ! <url>', 1]\n",
    "['840006160660983809', '<user> thanks for showing up for our appointment today . ', 1]\n",
    "\"\"\"\n",
    "\n",
    "N = 10\n",
    "# filename = \"train.cleaned\"\n",
    "# outfile  = \"processed_train_box_top{}\".format(N)\n",
    "def process(filename,outfile = \"\"):\n",
    "    with open(\"./data/text/{}.txt\".format(filename),'r',encoding='utf-8') as fin:\n",
    "        lines = fin.readlines()\n",
    "        lines = [x.strip() for x in lines]\n",
    "        # lines = lines[:200]\n",
    "        dic = dict()\n",
    "        for i in tqdm(range(len(lines))):\n",
    "            line = lines[i]\n",
    "            data = eval(line)\n",
    "            \n",
    "            if 'train' in filename:\n",
    "                id_, text, label = data\n",
    "            else:\n",
    "                id_, text, label1, label = data\n",
    "            if id_ in boxes:\n",
    "                # print(data)\n",
    "                box_text = list(boxes[id_].items())\n",
    "                \n",
    "                box_text_tmp = []\n",
    "                for key,val in box_text:\n",
    "                    key = get_real_label(key)\n",
    "                    box_text_tmp.append((\" \".join(key.split(\"_\")[:-1]), classes_to_index[key.split(\"_\")[-1]], val))\n",
    "                box_text = box_text_tmp\n",
    "                del box_text_tmp\n",
    "                \n",
    "                box_text = sorted(box_text,key = lambda x:x[1])\n",
    "                box_text = [(x,z) for x,y,z in box_text]\n",
    "\n",
    "                box_text = box_text[:N]\n",
    "                box_vit = [x[1][0] for x in box_text]\n",
    "                \n",
    "                text = keep_one_space(text) # 预处理，删除多余的空格，每个单词中间只留一个空格\n",
    "                \n",
    "                box_token = [tokenizer.tokenize(text.lower()) for text,_ in box_text]\n",
    "\n",
    "                text_graph,tokens,flag = generate_graph(text.lower())\n",
    "                image_graph,flags,word = generate_image_graph(text.lower(),[x[0] for x in box_text],flag)\n",
    "                \n",
    "                # flag == True   ->  spacy_token \n",
    "                # flag == False  ->  bert_token\n",
    "                \n",
    "                dic[id_] = {\n",
    "                    'text':text,\n",
    "                    'label':int(label),\n",
    "                    'tokens':tokens,\n",
    "                    'graph':text_graph,\n",
    "                    'box_tokens':box_token,\n",
    "                    'box_vit':box_vit,\n",
    "                    'image_graph':image_graph, #value['graph'] + value[\"sentic_graph\"]\n",
    "                    \"flags\":flags,\n",
    "                    \"flag\":flag,\n",
    "                    'word':word,\n",
    "                    \"box_text\":box_text\n",
    "                }\n",
    "\n",
    "                \n",
    "                # print(text,tokens, word)\n",
    "        print(\"{} : {} \".format(filename,len(dic)))\n",
    "        pickle.dump(dic,open(f\"{outfile}.data\",'wb'))\n",
    "\n",
    "\n",
    "process(\"train.cleaned\",\"/root/autodl-tmp/processed_train_box_top{}\".format(N))\n",
    "process(\"test.cleaned\",\"/root/autodl-tmp/processed_test_box_top{}\".format(N))\n",
    "process(\"valid.cleaned\",\"/root/autodl-tmp/processed_valid_box_top{}\".format(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65a75b74-214d-42d4-92fb-5c905b3dc3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29040 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0389, -0.1254, -0.1880,  ..., -0.1860,  0.1868, -0.1006])\n",
      "tensor([0.0389])\n",
      "tensor([-0.2407, -0.0252, -0.4480,  ..., -0.4536,  0.1642, -1.1719])\n",
      "tensor([-0.2407])\n",
      "tensor([ 0.1094,  0.0883, -0.1254,  ..., -0.1236,  0.2345,  0.2256])\n",
      "tensor([0.1094])\n",
      "tensor([-0.4438,  0.1112, -0.6410,  ..., -0.6592, -0.2009, -1.4022])\n",
      "tensor([-0.4438])\n",
      "tensor([0.4070, 0.3969, 0.1934,  ..., 0.1958, 0.4265, 0.5157])\n",
      "tensor([0.4070])\n",
      "tensor([ 0.1299,  0.1064, -0.0962,  ..., -0.0941,  0.4523, -0.0670])\n",
      "tensor([0.1299])\n",
      "tensor([ 0.3860,  0.2687,  0.1727,  ...,  0.1713, -0.1640, -0.0461])\n",
      "tensor([0.3860])\n",
      "tensor([ 0.2770, -0.0342,  0.0583,  ...,  0.0515, -0.1309,  0.4865])\n",
      "tensor([0.2770])\n",
      "tensor([-0.2115, -0.1467, -0.3990,  ..., -0.3971, -0.1272, -0.1456])\n",
      "tensor([-0.2115])\n",
      "tensor([-0.3630, -0.3123, -0.5732,  ..., -0.5963, -0.0627, -0.8018])\n",
      "tensor([-0.3630])\n",
      "[tensor(0.0389), tensor(-0.2407), tensor(0.1094), tensor(-0.4438), tensor(0.4070), tensor(0.1299), tensor(0.3860), tensor(0.2770), tensor(-0.2115), tensor(-0.3630)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/29040 [00:02<1:37:21,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1653,  0.1291, -0.4402,  ..., -0.4367,  0.1336, -0.4129])\n",
      "tensor([-0.1653])\n",
      "tensor([ 0.0162, -0.2315, -0.2202,  ..., -0.2361, -0.2290,  0.5811])\n",
      "tensor([0.0162])\n",
      "tensor([-0.0784, -0.2372, -0.2613,  ..., -0.2857, -0.2107, -0.5655])\n",
      "tensor([-0.0784])\n",
      "tensor([-0.1821, -0.4282, -0.3589,  ..., -0.3830,  0.0472, -0.0353])\n",
      "tensor([-0.1821])\n",
      "tensor([-0.2392, -0.5626, -0.4309,  ..., -0.4441, -0.3642, -0.3220])\n",
      "tensor([-0.2392])\n",
      "tensor([-0.0189,  0.2603, -0.2337,  ..., -0.2390,  0.0012,  0.2118])\n",
      "tensor([-0.0189])\n",
      "tensor([-0.0142,  0.0452, -0.1858,  ..., -0.2003,  0.0173, -0.5138])\n",
      "tensor([-0.0142])\n",
      "tensor([-0.2560, -0.4785, -0.4394,  ..., -0.4516, -1.1090, -0.5965])\n",
      "tensor([-0.2560])\n",
      "tensor([ 0.0486, -0.1447, -0.1712,  ..., -0.1961, -0.4140, -0.4611])\n",
      "tensor([0.0486])\n",
      "tensor([-0.1917, -0.4392, -0.3953,  ..., -0.4086, -0.3131,  0.5150])\n",
      "tensor([-0.1917])\n",
      "[tensor(-0.1653), tensor(0.0162), tensor(-0.0784), tensor(-0.1821), tensor(-0.2392), tensor(-0.0189), tensor(-0.0142), tensor(-0.2560), tensor(0.0486), tensor(-0.1917)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/29040 [00:03<1:27:12,  5.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34366/517105173.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.cleaned\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/root/autodl-tmp/processed_train_box_top{}111\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.cleaned\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/root/autodl-tmp/processed_test_box_top{}111\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"valid.cleaned\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/root/autodl-tmp/processed_valid_box_top{}111\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_34366/517105173.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(filename, outfile)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mtext_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mimage_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_image_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbox_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# flag == True   ->  spacy_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_34366/2983233560.py\u001b[0m in \u001b[0;36mgenerate_image_graph\u001b[0;34m(text, box_texts, flag)\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                         \u001b[0msplit_link1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                         \u001b[0mtokens_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                         \u001b[0mtokens_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1018\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;31m# This typically happens if a component is not initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/spacy/pipeline/trainable_pipe.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/spacy/pipeline/tok2vec.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mtokvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mbatch_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTok2VecListener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlistener\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisteners\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0monly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/layers/with_array.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSeqT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_list_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/layers/with_array.py\u001b[0m in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray1i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mXf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mYf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dXf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdYs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mListXd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mListXd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/layers/residual.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0md_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/thinc/layers/maxout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape2f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnO\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape1f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnO\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape3f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "['910308516510011393', 'most # funny quotes : 21 snarky and # funny quotes  # # funnyquotes # hilariousquotes # humor # lol ... <url> …', 1]\n",
    "['725333760762363905', ' spurs # creativethinking ! <url>', 1]\n",
    "['840006160660983809', '<user> thanks for showing up for our appointment today . ', 1]\n",
    "\"\"\"\n",
    "\n",
    "N = 10\n",
    "# filename = \"train.cleaned\"\n",
    "# outfile  = \"processed_train_box_top{}\".format(N)\n",
    "def process(filename,outfile = \"\"):\n",
    "    with open(\"./data/text/{}.txt\".format(filename),'r',encoding='utf-8') as fin:\n",
    "        lines = fin.readlines()\n",
    "        lines = [x.strip() for x in lines]\n",
    "        # lines = lines[:200]\n",
    "        dic = dict()\n",
    "        for i in tqdm(range(len(lines))):\n",
    "            line = lines[i]\n",
    "            data = eval(line)\n",
    "            \n",
    "            if 'train' in filename:\n",
    "                id_, text, label = data\n",
    "            else:\n",
    "                id_, text, label1, label = data\n",
    "            if id_ in boxes:\n",
    "                # print(data)\n",
    "                box_text = list(boxes[id_].items())\n",
    "\n",
    "                box_text_tmp = []\n",
    "                for key,val in box_text:\n",
    "                    key = get_real_label(key)\n",
    "                    box_text_tmp.append((\" \".join(key.split(\"_\")[:-1]), classes_to_index[key.split(\"_\")[-1]], val))\n",
    "                box_text = box_text_tmp\n",
    "                del box_text_tmp\n",
    "                \n",
    "                box_text = sorted(box_text,key = lambda x:x[1])\n",
    "                box_text = [(x,z) for x,y,z in box_text]\n",
    "\n",
    "                box_text = box_text[:N]\n",
    "                for x in box_text:\n",
    "                    print(x[1])\n",
    "                    print(x[1][0:1])\n",
    "                box_vit = [x[1][0] for x in box_text]\n",
    "                print(box_vit)\n",
    "                \n",
    "                text = keep_one_space(text) # 预处理，删除多余的空格，每个单词中间只留一个空格\n",
    "                \n",
    "                box_token = [tokenizer.tokenize(text.lower()) for text,_ in box_text]\n",
    "\n",
    "                text_graph,tokens,flag = generate_graph(text.lower())\n",
    "                image_graph,flags,word = generate_image_graph(text.lower(),[x[0] for x in box_text],flag)\n",
    "                \n",
    "                # flag == True   ->  spacy_token \n",
    "                # flag == False  ->  bert_token\n",
    "                \n",
    "                dic[id_] = {\n",
    "                    'text':text,\n",
    "                    'label':int(label),\n",
    "                    'tokens':tokens,\n",
    "                    'graph':text_graph,\n",
    "                    'box_tokens':box_token,\n",
    "                    'box_vit':box_vit,\n",
    "                    'image_graph':image_graph, #value['graph'] + value[\"sentic_graph\"]\n",
    "                    \"flags\":flags,\n",
    "                    \"flag\":flag,\n",
    "                    'word':word,\n",
    "                    \"box_text\":box_text\n",
    "                }\n",
    "\n",
    "                \n",
    "                # print(text,tokens, word)\n",
    "        print(\"{} : {} \".format(filename,len(dic)))\n",
    "        pickle.dump(dic,open(f\"{outfile}.data\",'wb'))\n",
    "\n",
    "\n",
    "process(\"train.cleaned\",\"/root/autodl-tmp/processed_train_box_top{}111\".format(N))\n",
    "process(\"test.cleaned\",\"/root/autodl-tmp/processed_test_box_top{}111\".format(N))\n",
    "process(\"valid.cleaned\",\"/root/autodl-tmp/processed_valid_box_top{}111\".format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aec6d6-c615-4001-b359-e43282404880",
   "metadata": {},
   "source": [
    "`dic`保存格式如下：\n",
    "\n",
    "```\n",
    "{\n",
    "    '687826579599196160': {\n",
    "        'text': 'we must rebuild our military ! ! ! we need more battleships ! # gopdebate', \n",
    "        'label': 1, \n",
    "        'tokens': [\n",
    "                    '[CLS]', 'we', 'must', 'rebuild', 'our', 'military', \n",
    "                    '!', '!', '!', 'we', 'need', 'more', 'battleships', \n",
    "                    '!', '#', 'gopdebate', '[SEP]'\n",
    "                ], \n",
    "        'graph': array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0.],\n",
    "                        [0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0.],\n",
    "                        ...\n",
    "                        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 1.]], \n",
    "                        dtype=float32),\n",
    "        'box_tokens': [['black'], ['white'], ['white'], ['white'], ['blue'], ['black'], ['black']], \n",
    "        'box_vit': [\n",
    "                    tensor(-0.2899), tensor(-0.2732), tensor(-0.3721), tensor(-0.2054), \n",
    "                    tensor(-0.2627), tensor(-0.3396), tensor(-0.6425)\n",
    "                ], \n",
    "        'image_graph': array([\n",
    "            [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
    "            [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
    "            [0.        , 0.08333334, 0.14285715, 0.14285715, 0.14285715, 0.08333334, 0.08333334, 0.08333334, 0.        ],\n",
    "           ...\n",
    "            [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ]], \n",
    "            dtype=float32), \n",
    "        'flags': [\n",
    "            False, False, False, False, False, \n",
    "            False, False, False, False, False, \n",
    "            False, False, False, False, False, \n",
    "            False, False, False, False, False, \n",
    "            False, False, False, False, False, \n",
    "            False, False, False, False, False, \n",
    "            False, False, False, False, False, \n",
    "            False, False, False, False, False, \n",
    "            False, False], \n",
    "        'flag': True, \n",
    "        'word': [], \n",
    "        'box_text': [\n",
    "            ('black', tensor([-0.2899,  0.1239, -0.4740,  ..., -0.4807, -0.4082, -0.6435])), \n",
    "            ('white', tensor([-0.2732,  0.4018, -0.4781,  ..., -0.4782, -0.4708, -0.2510])), \n",
    "            ('white', tensor([-0.3721,  0.1859, -0.5632,  ..., -0.5610, -0.2828, -0.3253])), \n",
    "            ('white', tensor([-0.2054,  0.3215, -0.4104,  ..., -0.4107, -0.2413, -0.2379])), \n",
    "            ('blue', tensor([-0.2627,  0.2279, -0.4667,  ..., -0.4689, -0.3917,  0.1752])), \n",
    "            ('black', tensor([-0.3396,  0.0095, -0.5434,  ..., -0.5445, -0.7224,  0.1304])), \n",
    "            ('black', tensor([-0.6425, -0.8791, -0.8490,  ..., -0.8507, -0.6838, -0.2024]))\n",
    "        ]\n",
    "    }, \n",
    "    '691915808591712257': {\n",
    "        'text': 'yep those penalty rates are a killer . ',\n",
    "        ...\n",
    "        'word': [], \n",
    "        'box_text': [\n",
    "            ('blue', tensor([-0.0349,  0.5834, -0.2572,  ..., -0.2592, -0.4214,  0.2335])), \n",
    "            ('', tensor([-0.5933,  0.4554, -0.7719,  ..., -0.7807,  0.1455, -0.3950])), \n",
    "            ('blue', tensor([-0.0630,  0.7266, -0.2437,  ..., -0.2503, -0.1166, -0.1800])), \n",
    "            ('', tensor([-0.2423,  0.6497, -0.4318,  ..., -0.4427, -0.1593, -0.4385])),\n",
    "            ('glass', tensor([-0.0430, -0.5507, -0.2726,  ..., -0.2768, -0.8849, -0.3752])), \n",
    "            ('black', tensor([-0.2212,  0.7649, -0.4232,  ..., -0.4304, -0.3663,  0.0212]))\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df419415-6d9a-42cf-a079-267785a81759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd83bd-9ea1-416d-8c36-69a43f9a60de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc42606-bf42-4cea-9577-e0658844ed35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
